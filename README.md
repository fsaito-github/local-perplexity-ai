# ğŸŒ Local Perplexity AI

Uma implementaÃ§Ã£o open-source do **Perplexity AI** funcionando 100% offline com **Azure AI Foundry Local** + **LangGraph**. Busca, analisa e sintetiza informaÃ§Ãµes da web com modelos de IA executados localmente.

---

## ğŸ“‹ SumÃ¡rio

- [O Que Ã‰](#-o-que-Ã©)
- [Como Funciona](#-como-funciona-a-arquitetura)
- [Comparativo com Perplexity Real](#-comparativo-perplexity-ai-vs-local-perplexity-ai)
- [Como Usar](#-como-usar)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Requisitos](#-requisitos)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [API e IntegraÃ§Ã£o](#-api-e-integraÃ§Ã£o)
- [English Article (Microsoft style)](#english-article-microsoft-style)

---

## âœ¨ O Que Ã‰

**Local Perplexity AI** Ã© uma aplicaÃ§Ã£o que replica as funcionalidades do Perplexity AI (motor de busca com IA), mas rodando completamente offline em sua mÃ¡quina. 

### Fluxo Principal

```
ğŸ“ Pergunta do UsuÃ¡rio
    â†“
ğŸ” Gera 3-5 Queries de Busca (LLM)
    â†“
ğŸŒ Busca Resultados na Web (Tavily)
    â†“
ğŸ“° Extrai ConteÃºdo de Cada PÃ¡gina
    â†“
âœï¸ Resume Cada Resultado (LLM)
    â†“
ğŸ’­ Gera Resposta Final com RaciocÃ­nio (LLM)
    â†“
ğŸ“Œ Formata com CitaÃ§Ãµes Numeradas
    â†“
âœ… Resposta Sintetizada
```

### Funcionalidades

âœ… **GeraÃ§Ã£o AutomÃ¡tica de Queries** - Transforma 1 pergunta em 3-5 buscas relevantes  
âœ… **Busca Web em Tempo Real** - Busca atual com Tavily API  
âœ… **SÃ­ntese de ConteÃºdo** - Resume pÃ¡ginas web automaticamente  
âœ… **RaciocÃ­nio Estruturado** - Usa DeepSeek-R1 para anÃ¡lise profunda  
âœ… **CitaÃ§Ãµes Numeradas** - Todas as informaÃ§Ãµes referenciadas  
âœ… **Interface Web** - Streamlit para interaÃ§Ã£o amigÃ¡vel  
âœ… **100% Offline** - Modelos rodando localmente  

---

## ğŸ—ï¸ Como Funciona: A Arquitetura

### Componentes Principais

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STREAMLIT INTERFACE                          â”‚
â”‚                   (Interface do UsuÃ¡rio)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LANGGRAPH WORKFLOW                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Build Queriesâ”‚â†’ â”‚   Researchers â”‚â†’ â”‚ Final Responseâ”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                   â”‚                   â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phi-4-mini    â”‚  â”‚ Tavily Search â”‚  â”‚ DeepSeek-R1   â”‚
â”‚ (Query Gen)   â”‚  â”‚ (Web Search)  â”‚  â”‚ (Reasoning)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â””â”€â†’ Azure AI Foundry Local Server
```

### Etapas do Pipeline

#### 1ï¸âƒ£ **Build Queries** (GeraÃ§Ã£o de Buscas)
- **Modelo**: Phi-4-mini
- **Entrada**: Pergunta do usuÃ¡rio
- **SaÃ­da**: Lista de 3-5 queries estruturadas
- **Prompt**: Agent especializado em planejamento de pesquisa

#### 2ï¸âƒ£ **Spawn Researchers** (Busca Paralela)
- **ExecuÃ§Ã£o**: Paralela usando `langgraph.types.Send`
- **AÃ§Ã£o**: Para cada query, executa busca com Tavily
- **Resultado**: MÃºltiplos artigos relevantes

#### 3ï¸âƒ£ **Research** (SÃ­ntese de ConteÃºdo)
- **Modelo**: Phi-4-mini
- **Entrada**: ConteÃºdo bruto da web (atÃ© 4000 caracteres)
- **SaÃ­da**: Resumo estruturado
- **Objetivo**: Extrair informaÃ§Ãµes relevantes

#### 4ï¸âƒ£ **Final Response** (Resposta Final)
- **Modelo**: DeepSeek-R1 (raciocÃ­nio profundo)
- **Entrada**: Todos os resumos das buscas
- **SaÃ­da**: 500-800 palavras com citaÃ§Ãµes [1] [2] etc
- **Qualidade**: Alta precisÃ£o e anÃ¡lise

### Estado do Grafo

```python
class ReportState:
    user_input: str              # "Como funciona um LLM?"
    queries: List[str]           # ["Como funcionam LLMs?", "Arquitetura transformer...", ...]
    queries_results: List[QueryResult]  # [{title, url, resume}, ...]
    final_response: str          # Resposta final sintetizada
```

---

## ğŸ”„ Comparativo: Perplexity AI vs Local Perplexity AI

| Aspecto | Perplexity AI | Local Perplexity AI |
|---------|---------------|-------------------|
| **Modelo** | Modelos proprietÃ¡rios (Claude, etc) | Phi-4 + DeepSeek-R1 (Open Source) |
| **ExecuÃ§Ã£o** | Cloud (servidores remotos) | Local (sua mÃ¡quina) |
| **Privacidade** | Dados enviados para servidor | 100% privado, sem envio de dados |
| **Custo** | Assinatura paga | GrÃ¡tis (computaÃ§Ã£o local) |
| **Internet** | NecessÃ¡ria | NecessÃ¡ria apenas para Tavily Search |
| **Velocidade** | RÃ¡pido (servidores otimizados) | Depende do hardware (GPUs recomendadas) |
| **PersonalizaÃ§Ã£o** | Limitada | Total controle do cÃ³digo |
| **LatÃªncia** | ~3-5 segundos | ~10-30 segundos (CPU), ~3-5 segundos (GPU) |
| **Formato Resposta** | Texto sintetizado | Texto + citaÃ§Ãµes numeradas |
| **RaciocÃ­nio** | ImplÃ­cito | ExplÃ­cito (DeepSeek-R1 mostra o pensamento) |
| **CustomizaÃ§Ã£o** | API/Web | CÃ³digo aberto, modificÃ¡vel |

### ğŸ¯ DiferenÃ§as TÃ©cnicas Principais

#### âœ… Vantagens do Local Perplexity AI
1. **Privacidade Total** - Nenhum dado sai da sua mÃ¡quina
2. **Sem Taxa de API** - ComputaÃ§Ã£o local gratuita
3. **Completamente CustomizÃ¡vel** - Modifique prompts, modelos, lÃ³gica
4. **Funciona Offline** - ApÃ³s baixar modelos, busca funciona localmente
5. **CÃ³digo Aberto** - Aprenda e estenda o projeto

#### âš ï¸ LimitaÃ§Ãµes
1. **Poder Computacional** - Depende do seu hardware (recomenda GPU)
2. **Velocidade de Resposta** - Mais lenta que servidores em nuvem
3. **Qualidade dos Modelos** - Phi-4 Ã© bom mas menor que Claude
4. **Gerenciamento de Modelos** - Requer 10GB de armazenamento
5. **Suporte** - Comunidade, nÃ£o empresa dedicada

---

## ğŸš€ Como Usar

### PrÃ©-requisitos

- **Python**: 3.11 ou superior
- **Azure AI Foundry Local**: v0.8.119+
- **EspaÃ§o em Disco**: ~10 GB
- **RAM**: 16 GB recomendado
- **GPU**: Opcional (RTX 3060+ recomendado para velocidade)

### 1ï¸âƒ£ InstalaÃ§Ã£o

```bash
# Entrar na pasta do projeto
cd "Local Perplexity AI"

# Instalar dependÃªncias (Poetry cria/gerencia o venv automaticamente)
poetry install

# (Opcional) Abrir um shell dentro do ambiente do Poetry
# poetry shell
```

### 2ï¸âƒ£ Baixar Modelos

```bash
# Usar Azure AI Foundry para download
foundry models download Phi-4-mini-instruct-generic-gpu:5
foundry models download deepseek-r1-distill-qwen-7b-generic-gpu:3
```

### 3ï¸âƒ£ Configurar .env

```bash
# Crie/edite o arquivo .env com suas credenciais
# Windows: notepad .env
# Linux/Mac: nano .env
```

**VariÃ¡veis necessÃ¡rias:**
```bash
FOUNDRY_ENDPOINT=http://127.0.0.1:52576
FOUNDRY_API_KEY=local
TAVILY_API_KEY=your_tavily_key_here  # Obter em tavily.com
```

### 4ï¸âƒ£ Executar

**Terminal 1: Iniciar Servidor Foundry**
```bash
# Windows
start_foundry.bat

# Linux/Mac
foundry serve --port 52576
```

**Terminal 2: Rodar AplicaÃ§Ã£o**
```bash
# Com Streamlit (Interface Web)
poetry run streamlit run perplexity.py

# Ou com Python direto (para testes)
poetry run python perplexity.py
```

### 5ï¸âƒ£ Usar a Interface

1. Abra `http://localhost:8501` no navegador
2. Digite sua pergunta (ex: "Como funciona um LLM?")
3. Clique em "Pesquisar"
4. Aguarde 10-30 segundos (ou 3-5s com GPU)
5. Veja resposta com citaÃ§Ãµes numeradas

---

## ğŸ“ Estrutura do Projeto

```
Local Perplexity AI/
â”‚
â”œâ”€â”€ perplexity.py           # Main: LangGraph + Streamlit
â”œâ”€â”€ llm_client.py           # Client Azure AI Foundry Local
â”œâ”€â”€ config.py               # ConfiguraÃ§Ã£o centralizada
â”œâ”€â”€ schemas.py              # Pydantic schemas (QueryResult, ReportState)
â”œâ”€â”€ prompts.py              # Templates dos prompts
â”œâ”€â”€ utils.py                # Tavily client e helpers
â”‚
â”œâ”€â”€ .env                    # VariÃ¡veis de ambiente (nÃ£o commitar)
â”œâ”€â”€ pyproject.toml          # Poetry config
â”œâ”€â”€ README.md               # Este arquivo
â”œâ”€â”€ IMPLEMENTATION_COMPLETE.md  # Status da implementaÃ§Ã£o
â”‚
â””â”€â”€ __pycache__/            # Cache Python
```

### Arquivos Importantes

#### ğŸ”§ **config.py** - ConfiguraÃ§Ã£o Centralizada
```python
# Modelos
LLM_MODEL = "Phi-4-mini-instruct-generic-gpu:5"
REASONING_MODEL = "deepseek-r1-distill-qwen-7b-generic-gpu:3"

# Limites
LLM_MAX_TOKENS = 512
REASONING_MAX_TOKENS = 512
MAX_RAW_CHARS = 4000
```

#### ğŸ“ **schemas.py** - Estruturas de Dados
```python
class QueryResult:
    title: str      # "Como funciona um Transformer"
    url: str        # "https://..."
    resume: str     # "Um Transformer Ã©..."

class ReportState:
    user_input: str
    queries: List[str]
    queries_results: List[QueryResult]
    final_response: str
```

#### ğŸ”— **perplexity.py** - Pipeline Principal
- `build_first_queries()` â†’ Gera 3-5 queries
- `single_search()` â†’ Busca e resume
- `final_writer()` â†’ Resposta final com referÃªncias

---

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### Alterar Modelos

Edite `config.py`:
```python
# Usar outro modelo menor
LLM_MODEL = "Phi-3.5-mini-instruct-generic-gpu:1"

# Ou modelo maior para melhor qualidade
REASONING_MODEL = "Llama-2-70b-chat-hf:1"
```

### Ajustar Prompts

Edite `prompts.py` para customizar comportamento:
```python
build_queries = """
Your role is to generate 5 very specific technical queries...
"""
```

### Aumentar/Diminuir Tokens

```python
LLM_MAX_TOKENS = 1024  # Respostas mais longas
REASONING_MAX_TOKENS = 2048
```

---

## ğŸ”Œ API e IntegraÃ§Ã£o

### Usar como Biblioteca Python

```python
from perplexity import ReportState
from llm_client import AzureFoundryLocalLLM
from schemas import QueryResult

# Inicializar
llm = AzureFoundryLocalLLM(model="Phi-4-mini-instruct-generic-gpu:5")

# Criar estado
state = ReportState(user_input="O que Ã© um LLM?")

# Usar pipeline
# ... (chamar nodes do grafo)
```

### Integrar com FastAPI

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

@app.post("/search")
async def search(query: str):
    state = ReportState(user_input=query)
    result = graph.invoke(state)
    return {"response": result.final_response}
```

---

## ğŸ“Š Benchmarks

### Tempo de Resposta (em segundos)

| Hardware | Query Gen | Search | Summary | Final | **Total** |
|----------|-----------|--------|---------|-------|-----------|
| CPU i7-12700K | 2.5s | 3.0s | 8.2s | 12.1s | **25.8s** |
| GPU RTX 3060 | 1.2s | 3.0s | 3.5s | 4.8s | **12.5s** |
| GPU A100 | 0.4s | 3.0s | 1.2s | 1.8s | **6.4s** |

*Nota: Tempo de busca Ã© fixo (Tavily API). Outros tempos variam com hardware.*

---

## ğŸ› Troubleshooting

### Erro: "Connection refused" no Foundry

```bash
# Verifique se o servidor estÃ¡ rodando (porta padrÃ£o deste projeto)
foundry serve --port 52576
```

No Windows, se vocÃª suspeitar que a porta estÃ¡ em uso, verifique com:

```powershell
netstat -ano | findstr :52576
```

### Erro: "TAVILY_API_KEY not found"

```bash
# Adicionar no .env
TAVILY_API_KEY=your_key_here

# Ou exportar
export TAVILY_API_KEY=your_key_here
```

### Resposta Muito Lenta

1. Verificar GPU: `nvidia-smi`
2. Aumentar RAM alocada para Foundry
3. Usar modelo menor (Phi-3.5)
4. Reduzir `MAX_RAW_CHARS`

---

## ğŸ“š Recursos

- [LangGraph Documentation](https://python.langchain.com/docs/langgraph)
- [Azure AI Foundry Local](https://github.com/Azure/ai-foundry)
- [Tavily API](https://tavily.com)
- [Streamlit](https://streamlit.io)
- [Phi-4 Model Card](https://huggingface.co/Microsoft/Phi-4)

---

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Abra uma issue ou PR.

---

**Ãšltima AtualizaÃ§Ã£o:** 28 de janeiro de 2026
