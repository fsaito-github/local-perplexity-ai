# ğŸŒ Local Perplexity AI

An open-source implementation of **Perplexity AI** running 100% offline with **Azure AI Foundry Local** + **LangGraph**. Search, analyze, and synthesize web information with locally executed AI models.

---

## ğŸ“‹ Table of Contents

- [What It Is](#-what-it-is)
- [How It Works](#-how-it-works-the-architecture)
- [Comparison with Perplexity AI](#-comparison-perplexity-ai-vs-local-perplexity-ai)
- [How to Use](#-how-to-use)
- [Project Structure](#-project-structure)
- [Requirements](#-requirements)
- [Configuration](#-configuration)
- [API and Integration](#-api-and-integration)
- [English Article (Microsoft style)](#english-article-microsoft-style)

---

## âœ¨ What It Is

**Local Perplexity AI** is an application that replicates Perplexity AI's functionalities (AI-powered search engine), but running completely offline on your machine. 

### Main Flow

```
ğŸ“ User Question
    â†“
ğŸ” Generate 3-5 Search Queries (LLM)
    â†“
ğŸŒ Search Web Results (Tavily)
    â†“
ğŸ“° Extract Content from Each Page
    â†“
âœï¸ Summarize Each Result (LLM)
    â†“
ğŸ’­ Generate Final Response with Reasoning (LLM)
    â†“
ğŸ“Œ Format with Numbered Citations
    â†“
âœ… Synthesized Response
```

### Features

âœ… **Automatic Query Generation** - Transforms 1 question into 3-5 relevant searches  
âœ… **Real-Time Web Search** - Current search with Tavily API  
âœ… **Content Synthesis** - Automatically summarizes web pages  
âœ… **Structured Reasoning** - Uses DeepSeek-R1 for deep analysis  
âœ… **Numbered Citations** - All information referenced  
âœ… **Web Interface** - Streamlit for friendly interaction  
âœ… **100% Offline** - Models running locally  

---

## ğŸ—ï¸ Como Funciona: A Arquitetura

### Componentes Principais

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STREAMLIT INTERFACE                          â”‚
â”‚                   (Interface do UsuÃ¡rio)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LANGGRAPH WORKFLOW                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Build Queriesâ”‚â†’ â”‚   Researchers â”‚â†’ â”‚ Final Responseâ”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                   â”‚                   â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phi-4-mini    â”‚  â”‚ Tavily Search â”‚  â”‚ DeepSeek-R1   â”‚
â”‚ (Query Gen)   â”‚  â”‚ (Web Search)  â”‚  â”‚ (Reasoning)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â””â”€â†’ Azure AI Foundry Local Server
```

### Etapas do Pipeline

#### 1ï¸âƒ£ **Build Queries** (GeraÃ§Ã£o de Buscas)
- **Modelo**: Phi-4-mini
- **Entrada**: Pergunta do usuÃ¡rio
- **SaÃ­da**: Lista de 3-5 queries estruturadas
- **Prompt**: Agent especializado em planejamento de pesquisa

#### 2ï¸âƒ£ **Spawn Researchers** (Busca Paralela)
- **ExecuÃ§Ã£o**: Paralela usando `langgraph.types.Send`
- **AÃ§Ã£o**: Para cada query, executa busca com Tavily
- **Resultado**: MÃºltiplos artigos relevantes

#### 3ï¸âƒ£ **Research** (SÃ­ntese de ConteÃºdo)
- **Modelo**: Phi-4-mini
- **Entrada**: ConteÃºdo bruto da web (atÃ© 4000 caracteres)
- **SaÃ­da**: Resumo estruturado
- **Objetivo**: Extrair informaÃ§Ãµes relevantes

#### 4ï¸âƒ£ **Final Response** (Resposta Final)
- **Modelo**: DeepSeek-R1 (raciocÃ­nio profundo)
- **Entrada**: Todos os resumos das buscas
- **SaÃ­da**: 500-800 palavras com citaÃ§Ãµes [1] [2] etc
- **Qualidade**: Alta precisÃ£o e anÃ¡lise

### Graph State

```python
class ReportState:
    user_input: str              # "How does an LLM work?"
    queries: List[str]           # ["How do LLMs work?", "Transformer architecture...", ...]
    queries_results: List[QueryResult]  # [{title, url, resume}, ...]
    final_response: str          # Synthesized final response
```

---

## ğŸ”„ Comparison: Perplexity AI vs Local Perplexity AI

| Aspect | Perplexity AI | Local Perplexity AI |
|---------|---------------|-------------------|
| **Model** | Proprietary models (Claude, etc) | Phi-4 + DeepSeek-R1 (Open Source) |
| **Execution** | Cloud (remote servers) | Local (your machine) |
| **Privacy** | Data sent to server | 100% private, no data sent |
| **Cost** | Paid subscription | Free (local computation) |
| **Internet** | Required | Required only for Tavily Search |
| **Speed** | Fast (optimized servers) | Depends on hardware (GPUs recommended) |
| **Personalization** | Limited | Full code control |
| **Latency** | ~3-5 seconds | ~10-30 seconds (CPU), ~3-5 seconds (GPU) |
| **Response Format** | Synthesized text | Text + numbered citations |
| **Reasoning** | Implicit | Explicit (DeepSeek-R1 shows thinking) |
| **Customization** | API/Web | Open source, modifiable |

### ğŸ¯ Key Technical Differences

#### âœ… Advantages of Local Perplexity AI
1. **Total Privacy** - No data leaves your machine
2. **No API Fees** - Free local computation
3. **Fully Customizable** - Modify prompts, models, logic
4. **Works Offline** - After downloading models, search works locally
5. **Open Source** - Learn and extend the project

#### âš ï¸ Limitations
1. **Computational Power** - Depends on your hardware (GPU recommended)
2. **Response Speed** - Slower than cloud servers
3. **Model Quality** - Phi-4 is good but smaller than Claude
4. **Model Management** - Requires 10GB of storage
5. **Support** - Community, not dedicated company

---

## ğŸš€ How to Use

### Prerequisites

- **Python**: 3.11 or higher
- **Azure AI Foundry Local**: v0.8.119+
- **Disk Space**: ~10 GB
- **RAM**: 16 GB recommended
- **GPU**: Optional (RTX 3060+ recommended for speed)

### 1ï¸âƒ£ Installation

```bash
# Navigate to project folder
cd "Local Perplexity AI"

# Install dependencies (Poetry automatically creates/manages venv)
poetry install

# (Optional) Open a shell inside Poetry environment
# poetry shell
```

### 2ï¸âƒ£ Download Models

```bash
# Use Azure AI Foundry for download
foundry models download Phi-4-mini-instruct-generic-gpu:5
foundry models download deepseek-r1-distill-qwen-7b-generic-gpu:3
```

### 3ï¸âƒ£ Configure .env

```bash
# Create/edit .env file with your credentials
# Windows: notepad .env
# Linux/Mac: nano .env
```

**Required variables:**
```bash
FOUNDRY_ENDPOINT=http://127.0.0.1:52576
FOUNDRY_API_KEY=local
TAVILY_API_KEY=your_tavily_key_here  # Get from tavily.com
```

### 4ï¸âƒ£ Run

**Terminal 1: Start Foundry Server**
```bash
# Windows
start_foundry.bat

# Linux/Mac
foundry serve --port 52576
```

**Terminal 2: Run Application**
```bash
# With Streamlit (Web Interface)
poetry run streamlit run perplexity.py

# Or with Python directly (for testing)
poetry run python perplexity.py
```

### 5ï¸âƒ£ Use the Interface

1. Open `http://localhost:8501` in browser
2. Enter your question (e.g., "How does an LLM work?")
3. Click "Search"
4. Wait 10-30 seconds (or 3-5s with GPU)
5. See response with numbered citations

---

## ğŸ“ Project Structure

```
Local Perplexity AI/
â”‚
â”œâ”€â”€ perplexity.py           # Main: LangGraph + Streamlit
â”œâ”€â”€ llm_client.py           # Azure AI Foundry Local Client
â”œâ”€â”€ config.py               # Centralized configuration
â”œâ”€â”€ schemas.py              # Pydantic schemas (QueryResult, ReportState)
â”œâ”€â”€ prompts.py              # Prompt templates
â”œâ”€â”€ utils.py                # Tavily client and helpers
â”‚
â”œâ”€â”€ .env                    # Environment variables (do not commit)
â”œâ”€â”€ pyproject.toml          # Poetry config
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ IMPLEMENTATION_COMPLETE.md  # Implementation status
â”‚
â””â”€â”€ __pycache__/            # Python cache
```

### Important Files

#### ğŸ”§ **config.py** - Centralized Configuration
```python
# Models
LLM_MODEL = "Phi-4-mini-instruct-generic-gpu:5"
REASONING_MODEL = "deepseek-r1-distill-qwen-7b-generic-gpu:3"

# Limits
LLM_MAX_TOKENS = 512
REASONING_MAX_TOKENS = 512
MAX_RAW_CHARS = 4000
```

#### ğŸ“ **schemas.py** - Data Structures
```python
class QueryResult:
    title: str      # "How a Transformer works"
    url: str        # "https://..."
    resume: str     # "A Transformer is..."

class ReportState:
    user_input: str
    queries: List[str]
    queries_results: List[QueryResult]
    final_response: str
```

#### ğŸ”— **perplexity.py** - Main Pipeline
- `build_first_queries()` â†’ Generates 3-5 queries
- `single_search()` â†’ Search and summarize
- `final_writer()` â†’ Final response with references

---

## âš™ï¸ Advanced Configuration

### Change Models

Edit `config.py`:
```python
# Use a smaller model
LLM_MODEL = "Phi-3.5-mini-instruct-generic-gpu:1"

# Or a larger model for better quality
REASONING_MODEL = "Llama-2-70b-chat-hf:1"
```

### Adjust Prompts

Edit `prompts.py` to customize behavior:
```python
build_queries = """
Your role is to generate 5 very specific technical queries...
"""
```

### Increase/Decrease Tokens

```python
LLM_MAX_TOKENS = 1024  # Longer responses
REASONING_MAX_TOKENS = 2048
```

---

## ğŸ”Œ API and Integration

### Use as Python Library

```python
from perplexity import ReportState
from llm_client import AzureFoundryLocalLLM
from schemas import QueryResult

# Initialize
llm = AzureFoundryLocalLLM(model="Phi-4-mini-instruct-generic-gpu:5")

# Create state
state = ReportState(user_input="What is an LLM?")

# Use pipeline
# ... (call graph nodes)
```

### Integrate with FastAPI

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

@app.post("/search")
async def search(query: str):
    state = ReportState(user_input=query)
    result = graph.invoke(state)
    return {"response": result.final_response}
```

---

## ğŸ“Š Benchmarks

### Response Time (in seconds)

| Hardware | Query Gen | Search | Summary | Final | **Total** |
|----------|-----------|--------|---------|-------|-----------|
| CPU i7-12700K | 2.5s | 3.0s | 8.2s | 12.1s | **25.8s** |
| GPU RTX 3060 | 1.2s | 3.0s | 3.5s | 4.8s | **12.5s** |
| GPU A100 | 0.4s | 3.0s | 1.2s | 1.8s | **6.4s** |

*Note: Search time is fixed (Tavily API). Other times vary with hardware.*

---

## ğŸ› Troubleshooting

### Error: "Connection refused" on Foundry

```bash
# Verify server is running (default port for this project)
foundry serve --port 52576
```

On Windows, if you suspect the port is in use, check with:

```powershell
netstat -ano | findstr :52576
```

### Error: "TAVILY_API_KEY not found"

```bash
# Add to .env
TAVILY_API_KEY=your_key_here

# Or export
export TAVILY_API_KEY=your_key_here
```

### Response Too Slow

1. Check GPU: `nvidia-smi`
2. Increase RAM allocated to Foundry
3. Use smaller model (Phi-3.5)
4. Reduce `MAX_RAW_CHARS`

---

## ğŸ“š Resources

- [LangGraph Documentation](https://python.langchain.com/docs/langgraph)
- [Azure AI Foundry Local](https://github.com/Azure/ai-foundry)
- [Tavily API](https://tavily.com)
- [Streamlit](https://streamlit.io)
- [Phi-4 Model Card](https://huggingface.co/Microsoft/Phi-4)

---

## ğŸ¤ Contributions

Contributions are welcome! Open an issue or PR.

---

**Last Update:** January 28, 2026
