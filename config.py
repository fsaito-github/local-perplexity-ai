"""Configura√ß√£o centralizada do projeto Local Perplexity AI"""

import os
import logging
import sys
from dotenv import load_dotenv

load_dotenv()

# ============================================================================
# AZURE FOUNDRY SETTINGS
# ============================================================================
FOUNDRY_ENDPOINT = os.getenv("FOUNDRY_ENDPOINT", "http://127.0.0.1:52576")
FOUNDRY_API_KEY = os.getenv("FOUNDRY_API_KEY", "local")

# ============================================================================
# MODELOS LLM
# ============================================================================
# Modelo principal para gerar queries e resumir resultados
LLM_MODEL = "Phi-4-mini-instruct-generic-gpu:5"
LLM_MAX_TOKENS = 512
LLM_TEMPERATURE = 0.7
LLM_TIMEOUT = 120

# Modelo para racioc√≠nio e gerar resposta final
REASONING_MODEL = "deepseek-r1-distill-qwen-7b-generic-gpu:3"
REASONING_MAX_TOKENS = 512
REASONING_TEMPERATURE = 0.3
REASONING_TIMEOUT = 300

# ============================================================================
# TAVILY SEARCH SETTINGS
# ============================================================================
TAVILY_MAX_RESULTS = 1
MAX_RAW_CHARS = 4000

# ============================================================================
# STREAMLIT SETTINGS
# ============================================================================
DEFAULT_QUERY = "How is the process of building a LLM?"
STREAMLIT_TITLE = "üåé Local Perplexity"

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'


def setup_logging(level: str = LOG_LEVEL) -> logging.Logger:
    """
    Configurar logging estruturado para o projeto.
    
    Args:
        level: N√≠vel de logging (DEBUG, INFO, WARNING, ERROR)
        
    Returns:
        Logger configurado
    """
    logger = logging.getLogger("perplexity")
    
    # N√£o adicionar handlers duplicados
    if logger.handlers:
        return logger
    
    logger.setLevel(getattr(logging, level))
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(getattr(logging, level))
    console_formatter = logging.Formatter(LOG_FORMAT)
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    # File handler
    file_handler = logging.FileHandler("perplexity.log")
    file_handler.setLevel(getattr(logging, level))
    file_formatter = logging.Formatter(LOG_FORMAT)
    file_handler.setFormatter(console_formatter)
    logger.addHandler(file_handler)
    
    return logger


# ============================================================================
# VALIDA√á√ÉO DE CONFIGURA√á√ÉO
# ============================================================================
def validate_config() -> bool:
    """
    Validar se as configura√ß√µes essenciais est√£o presentes.
    
    Returns:
        True se v√°lido, False caso contr√°rio
    """
    logger = setup_logging()
    
    # Validar endpoint do Foundry
    if not FOUNDRY_ENDPOINT:
        logger.error("‚ùå FOUNDRY_ENDPOINT n√£o est√° configurado")
        return False
    
    logger.info(f"‚úÖ Configura√ß√£o carregada com sucesso")
    logger.debug(f"  - Endpoint: {FOUNDRY_ENDPOINT}")
    logger.debug(f"  - Modelo Principal: {LLM_MODEL}")
    logger.debug(f"  - Modelo Racioc√≠nio: {REASONING_MODEL}")
    
    return True


# Inicializar logging ao importar o m√≥dulo
_logger = setup_logging()

__all__ = [
    "FOUNDRY_ENDPOINT",
    "FOUNDRY_API_KEY",
    "LLM_MODEL",
    "LLM_MAX_TOKENS",
    "LLM_TEMPERATURE",
    "LLM_TIMEOUT",
    "REASONING_MODEL",
    "REASONING_MAX_TOKENS",
    "REASONING_TEMPERATURE",
    "REASONING_TIMEOUT",
    "TAVILY_MAX_RESULTS",
    "MAX_RAW_CHARS",
    "DEFAULT_QUERY",
    "STREAMLIT_TITLE",
    "LOG_LEVEL",
    "setup_logging",
    "validate_config",
]
